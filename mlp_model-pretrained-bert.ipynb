{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import AUTOTUNE\n",
    "import sentiment\n",
    "\n",
    "\n",
    "DATASET_DIR = './goemotions/data-v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sentiment.load_classes(DATASET_DIR)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = sentiment.make_dataframes(DATASET_DIR,\n",
    "                                                      fraction=0.8,\n",
    "                                                      clean=True,\n",
    "                                                      cut_neutral=False,\n",
    "                                                      optimize_low=False)\n",
    "dataframes = {'Train': train_df, 'Validation': val_df, 'Test': test_df}\n",
    "for title, df in dataframes.items():\n",
    "    sentiment.plot_class_distr(df, classes, title)\n",
    "train_ds = sentiment.make_ts_ds(train_df, classes, BATCH_SIZE, AUTOTUNE)\n",
    "val_ds = sentiment.make_ts_ds(val_df, classes, BATCH_SIZE, AUTOTUNE)\n",
    "test_ds = sentiment.make_ts_ds(test_df, classes, BATCH_SIZE, AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement transorfmers (from versions: none)\n",
      "ERROR: No matching distribution found for transorfmers\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install transorfmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (Dropout, Dense)\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "classifier = Sequential([\n",
    "    encoder,\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(classes), activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "METRICS_THRESHOLD = 0.5\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "\n",
    "metrics = sentiment.create_metrics(classes, METRICS_THRESHOLD)\n",
    "loss = BinaryCrossentropy()\n",
    "classifier.compile(Adam(learning_rate=LEARNING_RATE), loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_counts = sentiment.get_class_counts(train_df)\n",
    "total = sum(class_counts)\n",
    "class_weights = {i: total / count for i, count in enumerate(class_counts)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "hist = classifier.fit(x=train_ds, validation_data=val_ds,\n",
    "                      validation_steps=30, epochs=EPOCHS,\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=3)]\n",
    "                      # class_weight=class_weights\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = classifier.evaluate(x=test_ds, return_dict=True)\n",
    "sentiment.print_metrics(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = hist.history\n",
    "stats_graps = {\n",
    "    'Val Loss': stats['val_loss'],\n",
    "    'Val Precision All': stats['precision@0.5/all'],\n",
    "    'Val Recall All': stats['recall@0.5/all'],\n",
    "    'Val F1-Score All': stats['f1_score@0.5/all']\n",
    "}\n",
    "sentiment.plot_history(stats_graps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.calc_accuracy(test_ds, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.test_examples(classifier, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.plot_conf_mtrx_all(classifier, test_ds, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.plot_conf_mtrx_per_class(classifier, test_ds, classes, rounded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('./models/mlp', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Experiment 1\n",
    "__Encoder:__ 20000, tf_df, ngram=2  \n",
    "__Dataset:__ BATCH_SIZE=128 fraction=0.8, cut_neutral=False, optimize_low=False  \n",
    "__Arch:__ 1 Dense layer with 128 units, dropout=0.2  \n",
    "__Learning:__ LR=1e-4, eary stop at 2  \n",
    "__Result:__\n",
    "F1Score all: `0.31189`  \n",
    "Accuracy: `0.4528551`  \n",
    "Grief and relief have zero metrics  \n",
    "  \n",
    "### Experiment 2\n",
    "__Encoder:__ 20000, tf_df, ngram=2  \n",
    "__Dataset:__ BATCH_SIZE = 128 fraction=0.8, cut_neutral=False, optimize_low=False  \n",
    "__Arch:__ 1 Dense layer with 128 units, dropout=0.2, ___added BatchNormalization layer before activation layer___  \n",
    "__Learning:__ LR=1e-4, eary stop at 2  \n",
    "__Result:__\n",
    "F1Score all: `0.29137`  \n",
    "Accuracy: `0.4734294`  \n",
    "Grief and nervousness have zero metrics \n",
    "\n",
    "### Experiment 3\n",
    "__Encoder:__ 20000, tf_df, ngram=2  \n",
    "__Dataset:__ BATCH_SIZE = 64 fraction=0.8, cut_neutral=False, optimize_low=False  \n",
    "__Arch:__ 1 Dense layer with 128 units, dropout=0.2  \n",
    "__Learning:__ LR=1e-4, eary stop at 2  \n",
    "__Result:__\n",
    "F1Score all: `0.2987`  \n",
    "Accuracy: `0.46627793`  \n",
    "__All classes have non-zero metrics__\n",
    "\n",
    "### Experiment 4\n",
    "__Encoder:__ 20000, tf_df, ngram=2  \n",
    "__Dataset:__ BATCH_SIZE = 64 fraction=0.8, cut_neutral=True, optimize_low=False  \n",
    "__Arch:__ 1 Dense layer with 128 units, dropout=0.2  \n",
    "__Learning:__ LR=1e-4, early stop at 2  \n",
    "__Result:__\n",
    "F1Score all: `0.3905`  \n",
    "Accuracy: `0.48134044`  \n",
    "Grief and relief have zero metrics\n",
    "\n",
    "### Experiment 5\n",
    "__Encoder:__ 20000, tf_df, ngram=2  \n",
    "__Dataset:__ BATCH_SIZE = 32 fraction=0.8, cut_neutral=False, optimize_low=False  \n",
    "__Arch:__ 1 Dense layer with 128 units, dropout=0.2  \n",
    "__Learning:__ LR=1e-4, early stop at 2  \n",
    "__Result:__\n",
    "F1Score all: `0.3167`  \n",
    "Accuracy: `0.47496974`  \n",
    "Grief has zero metrics \n",
    "\n",
    "### Experiment 6\n",
    "__Encoder:__ 20000, tf_df, ngram=2  \n",
    "__Dataset:__ BATCH_SIZE = 64 fraction=0.8, cut_neutral=False, optimize_low=False  \n",
    "__Arch:__ 1 Dense layer with 128 units, dropout=0.2  \n",
    "__Learning:__ LR=1e-4, early stop at 3  \n",
    "__Result:__\n",
    "F1Score all: `0.33037`  \n",
    "Accuracy: `0.44658378`  \n",
    "Grief has zero metrics \n",
    "\n",
    "### Experiment 7\n",
    "__Encoder:__ 20000, tf_df, ngram=2  \n",
    "__Dataset:__ BATCH_SIZE = 64 fraction=0.8, cut_neutral=False, optimize_low=False  \n",
    "__Arch:__ 1 Dense layer with 128 units, dropout=0.2  \n",
    "__Learning:__ LR=1e-4, early stop at 5  \n",
    "__Result:__\n",
    "F1Score all: `0.3370`  \n",
    "Accuracy: `0.44746396`  \n",
    "__All classes have non-zero metrics__  \n",
    "\n",
    "### Experiment 8\n",
    "__Encoder:__ 20000, tf_df, ngram=2  \n",
    "__Dataset:__ BATCH_SIZE = 64 fraction=0.8, cut_neutral=False, optimize_low=False  \n",
    "__Arch:__ 1 Dense layer with 128 units, dropout=0.2  \n",
    "__Learning:__ LR=1e-4, early stop at 10  \n",
    "__Result:__\n",
    "F1Score all: `0.35816`  \n",
    "Accuracy: `0.45252502`  \n",
    "Relief has zero metrics   \n",
    "\n",
    "### Experiment 8\n",
    "__Encoder:__ 20000, tf_df, ngram=2  \n",
    "__Dataset:__ BATCH_SIZE = 64 fraction=0.8, cut_neutral=False, optimize_low=False  \n",
    "__Arch:__ 1 Dense layer with 128 units, dropout=0.2  \n",
    "__Learning:__ LR=1e-4, no early stop, 50 epochs  \n",
    "__Result:__\n",
    "F1Score all: `0.38314`  \n",
    "Accuracy: `0.43206072`  \n",
    "Relief has zero metrics    \n",
    "\n",
    "### Experiment 9 - WITH CLEANING\n",
    "__Encoder:__ 20000, tf_df, ngram=(1, 2)  \n",
    "__Dataset:__ BATCH_SIZE = 64 fraction=0.8, cut_neutral=True, optimize_low=False  \n",
    "__Arch:__ 1 Dense layer with 128 units, dropout=0.2  \n",
    "__Learning:__ LR=1e-4, early stop at 3, 100 epochs, no class weights    \n",
    "__Result:__\n",
    "F1Score all: `0.41347`  \n",
    "Accuracy: `0.4923077`  \n",
    "Grief, pride, relief has zero metrics    \n",
    "# Without cut_neutral performed better, but slightly overfit. Try the same without class weights\n",
    "\n",
    "---\n",
    "_Using both optimize_low and class_weights all metrics have non-zero value, but total result is lower. accuracy is still not high enough.  \n",
    "Try with maxsequence  \n",
    "Try with Learning Rate exponential decay  \n",
    "F1-Score weights?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
